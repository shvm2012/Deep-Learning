{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_classification_TFIDF_vectorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shvm2012/Natural-Language-Processing-essentials/blob/master/Sentiment_classification_TFIDF_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA7yB6r_mlTe",
        "colab_type": "text"
      },
      "source": [
        "## Useful imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pgc5tG_TKykg",
        "colab": {}
      },
      "source": [
        "# Import 'os' for preliminary tasks like directory listing etc.\n",
        "import os\n",
        "\n",
        "# Import re for regex string matching\n",
        "import re\n",
        "\n",
        "# Import nltk for nlp\n",
        "import nltk\n",
        "\n",
        "# Import library providing high-performance, easy-to-use data structures and data analysis tools\n",
        "import pandas as pd\n",
        "\n",
        "# Import Python's native data structures Counter and defaultdict\n",
        "# Counter - maintains count of element\n",
        "# defaultdict - dictionary data structure with exception handling for missing keys\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Import tqdm for fancy progressbars!\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "# Import numpy for different mathematical operations on arrays / matrices\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize # import tokenizer\n",
        "from nltk.corpus import stopwords # import stopwords\n",
        "from nltk.stem.porter import PorterStemmer #import stemmer\n",
        "from nltk.stem import WordNetLemmatizer #lemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # for text to vector\n",
        "from sklearn.naive_bayes import MultinomialNB #import naive bayes classifier\n",
        "from sklearn import svm #import SVM classifier\n",
        "from  sklearn.metrics  import accuracy_score # accuracy measure\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GqYEfPkiNpvc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "e5b4ec64-61d6-43c5-8834-98ffe8abc63f"
      },
      "source": [
        "# Install the nltk component for several tasks\n",
        "nltk.download('punkt')     \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DCRqGNHWSW2",
        "colab_type": "text"
      },
      "source": [
        "## NLP (nltk) basics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6dc977d9-ff09-4135-bf56-03012b0eed3e",
        "id": "mkq2W6izV0gy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#sentence for testing\n",
        "sentence= \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "#function to split text into word\n",
        "tokens = word_tokenize(sentence)\n",
        "print (tokens)\n",
        "\n",
        "#POS_Tagging\n",
        "nltk.pos_tag(tokens)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('quick', 'JJ'),\n",
              " ('brown', 'NN'),\n",
              " ('fox', 'NN'),\n",
              " ('jumps', 'VBZ'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('lazy', 'JJ'),\n",
              " ('dog', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEqtlOcPWLh3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7c3416e0-5712-4c7d-a105-6bb835e5f6a3"
      },
      "source": [
        "#stop words removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(list(stop_words)[:10]) #some common stopwords\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "print(tokens)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['both', 'yourselves', 'won', \"you've\", 'm', 'most', 'are', \"hasn't\", 'myself', 'so']\n",
            "['The', 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOUP3zbXZRK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "693e01a5-d646-41d5-893f-27d28cd52f58"
      },
      "source": [
        "# stemming\n",
        "porter = PorterStemmer()\n",
        "stems = []\n",
        "for t in tokens:    \n",
        "    stems.append(porter.stem(t))\n",
        "print(stems)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga4oSKjrZgUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "69652dbf-7cc5-47d2-8489-a717d349b8d9"
      },
      "source": [
        "#lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas=[]\n",
        "for t in tokens:\n",
        "  lemmas.append(lemmatizer.lemmatize(t))\n",
        "print (lemmas)\n",
        "\n",
        "print(\"better:\", lemmatizer.lemmatize(\"better\",pos =\"a\" ))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n",
            "better: good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7rskn91gKykl"
      },
      "source": [
        "## Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0vFdDGbKykm",
        "outputId": "df96db32-64d8-40de-a0d5-c10139e9e511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-19 17:11:35--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘data/aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  22.9MB/s    in 4.3s    \n",
            "\n",
            "2019-09-19 17:11:40 (18.6 MB/s) - ‘data/aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6m_dPDAoKykp",
        "outputId": "422eea3b-391b-4aab-ab0e-0c950cf78126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "## Data extraction\n",
        "%%time\n",
        "!tar -xzf data/aclImdb_v1.tar.gz -C data/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 83.7 ms, sys: 11.3 ms, total: 95 ms\n",
            "Wall time: 10.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcRfn_pKRUTc",
        "colab_type": "text"
      },
      "source": [
        "### Data Samples\n",
        "- Dataset is split into two parts for training and testing\n",
        "- Positive and negative samples are organized in individual folders \n",
        "- Each sample document is stored in a .txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncwqIYpHeHHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the dataset from files to a python DataFrame\n",
        "folder = 'data/aclImdb/'\n",
        "labels = {'pos': 1, 'neg': 0} \n",
        "df = pd.DataFrame()\n",
        "revList = list()\n",
        "for f in ('test', 'train'):    \n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(folder, f, l)\n",
        "        for file in os.listdir (path) :\n",
        "            with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "                revList.append((txt,labels[l]))\n",
        "            #df = df.append([[txt, labels[l]]],ignore_index=True)\n",
        "df = pd.DataFrame.from_records(revList)\n",
        "df.columns = ['review', 'sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NMS_5LAYSBz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "e61b7bb0-1854-4dcb-9bc8-9f0d55031996"
      },
      "source": [
        "## peeking at the data\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Each frame in the movie is a lesson to new dir...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ken Loach showed the world the down-and-out fl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It may be hard to explain how, but this film i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Had it with the one who raised you since when ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A photographer in the small city of Gunsan in ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  Each frame in the movie is a lesson to new dir...          1\n",
              "1  Ken Loach showed the world the down-and-out fl...          1\n",
              "2  It may be hard to explain how, but this film i...          1\n",
              "3  Had it with the one who raised you since when ...          1\n",
              "4  A photographer in the small city of Gunsan in ...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9tpW3mXYX2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6697159c-1a6c-4857-9fa8-7a5b7ce0c41b"
      },
      "source": [
        "df.shape ## total 50k reviews"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWvbRkPNYea5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cf9d29e4-57a7-42a8-fc42-21214407fd74"
      },
      "source": [
        "df.sentiment.value_counts() ## 25k positive reviews (label : 1) and 25k negative (label : 0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    25000\n",
              "0    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXSN5c7bRcBL",
        "colab_type": "text"
      },
      "source": [
        "## Build Vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQTKbG7eY6Kw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9dffc853-ef24-4f8b-fe2d-74c1099ca205"
      },
      "source": [
        "reviews = df.review.str.cat(sep=' ') ## storing all reviews in a tab seperated string "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65521550"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SJtY9mw6Kykv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60ba090e-bcb8-4172-9a54-6d3dbb0b3886"
      },
      "source": [
        "#function to split text into word\n",
        "tokens = word_tokenize(reviews)\n",
        "vocabulary = set(tokens)\n",
        "print(len(vocabulary)) ## number of words in the vocabulary"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rtmF1HXr4l5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3b6af27-3248-4a8b-929e-a73ce7636965"
      },
      "source": [
        "## removing stopwords from vocabulary\n",
        "stop_words = set(stopwords.words('english'))\n",
        "vocabulary = [w for w in vocabulary if not w in stop_words]\n",
        "print(len(vocabulary))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "199634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFRztCa2Rn0P",
        "colab_type": "text"
      },
      "source": [
        "## Preparing data for modeling (tfidf vectorization)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_hUNsOga928",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building a classifier\n",
        "X_train = df.loc[:24999, 'review'].values\n",
        "y_train = df.loc[:24999, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3GZnu0UbCjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea4287b5-2da6-493a-e2fa-43b48e38694e"
      },
      "source": [
        "sum(y_train==1) #25k training data reviews (12.5k positive & 12.5k negative)\n",
        "sum(y_test==1) #25k  test data reviews (12.5k positive & 12.5k negative)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEzTwRrMdHMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer() ## for making of document(reviews)-term matrix(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E63RiYjt3ZMr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "675c7cbc-b3b2-425a-ba3c-afbd2007ddf2"
      },
      "source": [
        "train_vectors=vectorizer.fit_transform(X_train)\n",
        "test_vectors = vectorizer.transform(X_test)\n",
        "print(train_vectors.shape, test_vectors.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 73822) (25000, 73822)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVPKJ1CxfzJk",
        "colab_type": "text"
      },
      "source": [
        "## Building Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkkkmBivdu4s",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Fitting Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgzM1ceGsuZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d45a9d2f-dc7e-4569-da7e-95018077d05b"
      },
      "source": [
        "clf = MultinomialNB().fit(train_vectors, y_train)\n",
        "predicted = clf.predict(test_vectors)\n",
        "print(\"Naive Bayes Accuracy:\", np.round(accuracy_score(y_test,predicted),2))\n",
        "print(\"Naive Bayes F1 score:\", np.round(f1_score(y_test,predicted),2))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy: 0.84\n",
            "Naive Bayes F1 score: 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT0lmSTaeagf",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Fitting Random Forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZDEHP7ZtjmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fe5dcdd8-f38f-442e-fcc2-4c21be0957e5"
      },
      "source": [
        "clf=RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1).fit(train_vectors, y_train)\n",
        "predicted = clf.predict(test_vectors)\n",
        "print(\"Random Forest Accuracy:\", np.round(accuracy_score(y_test,predicted),2))\n",
        "print(\"Random Forest F1 score:\", np.round(f1_score(y_test,predicted),2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.53\n",
            "Random Forest F1 score: 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWozfpYifIVR",
        "colab_type": "text"
      },
      "source": [
        "Similarly we can fit any classifer of our choice on this IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TcaT4_tkKylc",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}